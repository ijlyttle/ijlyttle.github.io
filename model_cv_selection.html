<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Model-selection using cross-validation</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="site_libs/highlight/default.css"
      type="text/css" />
<script src="site_libs/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>

<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("show" === "show");
});
</script>




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.9em;
  padding-left: 5px;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Wanderings through the Tidyverse</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Dimensions
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Ingest</li>
    <li class="dropdown-header">Tidy</li>
    <li class="dropdown-header">Transform</li>
    <li class="dropdown-header">Visualize</li>
    <li class="dropdown-header">Model</li>
    <li>
      <a href="model_introduction.html">Introduction</a>
    </li>
    <li>
      <a href="model_cv_selection.html">Model-selection using cross-validation</a>
    </li>
    <li>
      <a href="model_bootstrap_parameter.html">Parameter-assessment using bootstrapping</a>
    </li>
    <li class="dropdown-header">Communicate</li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Non-dimensions
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Introduction</li>
    <li class="dropdown-header">Dates and times</li>
    <li class="dropdown-header">API-client design</li>
    <li class="dropdown-header">Makefiles for project manangement</li>
    <li class="dropdown-header">Custom R Markdown formats</li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Package portfolio
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">vembedr</li>
    <li class="dropdown-header">bsplus</li>
    <li class="dropdown-header">shinypod</li>
    <li class="dropdown-header">humidr</li>
    <li class="dropdown-header">groundTemp</li>
  </ul>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">

<div class="btn-group pull-right">
<button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Model-selection using cross-validation</h1>

</div>


<pre class="r"><code>library(&quot;devtools&quot;)
library(&quot;tibble&quot;)
library(&quot;ggplot2&quot;)
library(&quot;modelr&quot;)
library(&quot;dplyr&quot;)
library(&quot;purrr&quot;)
library(&quot;tidyr&quot;)
library(&quot;pryr&quot;)

knitr::opts_chunk$set(comment = &quot;&quot;) # put this into a format, once developed</code></pre>
<p>There are three type of things we want to do:</p>
<ol style="list-style-type: decimal">
<li><p>For a given model, make an estimate of its performance. We will do this using cross-validation, employing a number of different random train/test splits; the estimate of performance for a given model will be an aggregation of the performance of each of the splits.</p></li>
<li><p>Evaluation of the performance of a number of models - each model will have a different set of features. It is thought that, at first, that all of the models evaluated as a set will have the same family, i.e. <code>lm()</code>, but this may not always be the case, as we may be further interested in making a selection from models of different families.</p></li>
<li><p>For a given model (such as the chosen one), we want to make an estimate of how much confidence we should have in the significance of each of its coefficients.</p></li>
</ol>
<p>For the first two, we will use cross-validation - for the third we will use boostrapping.</p>
<div id="linear-regression" class="section level2">
<h2>Linear regression</h2>
<p>Following Hadley’s example, we want to create a dataset using the “truth” and “noise” that we specify.</p>
<p>The truth function is a simple quadratic:</p>
<pre class="r"><code>truth &lt;- function(x){
  1 + 2*x - x^2
}</code></pre>
<p>The noise function is a normal distribution with a nominal standard-deviation.</p>
<pre class="r"><code>noise &lt;- function(x){
  rnorm(length(x), sd = 0.1)
}</code></pre>
<p>We assemble a dataset using a random uniform distribution in <span class="math inline">\(x\)</span>; our <span class="math inline">\(y\)</span> is the sum of truth and noise.</p>
<pre class="r"><code>df &lt;- 
  data_frame(
    x = runif(n = 100, min = 0, max = 1),
    y = truth(x) + noise(x)
  ) %&gt;%
  print()</code></pre>
<pre><code># A tibble: 100 x 2
           x        y
       &lt;dbl&gt;    &lt;dbl&gt;
1  0.5210547 1.855529
2  0.4627843 1.573038
3  0.5244896 1.677966
4  0.9891559 1.947288
5  0.5416644 1.835031
6  0.6756441 1.889039
7  0.3519536 1.962250
8  0.5697320 1.759407
9  0.7733154 1.918960
10 0.2193805 1.339989
# ... with 90 more rows</code></pre>
<p>Of course, our first step is to visualize the dataset.</p>
<pre class="r"><code>ggplot(df, aes(x = x, y = y)) +
  geom_point(alpha = 0.6)</code></pre>
<p><img src="model_cv_selection_files/figure-html/scatter-1.png" width="672" /></p>
<p>This is a special case where we have a single independent variable. This is great for showing the concepts, but I am interested to explore visual tools that I can use to assess models that use multiple independent variables. It seems that focusing on the residual might be a useful place to start.</p>
<div id="evaluating-a-single-model" class="section level3">
<h3>Evaluating a single model</h3>
<p>We will start by following Hadley and Garrett, building a dataframe that to contain a number of random splits of our original data into a data to train a model, and data to test a model. We use the the <code>crossv_mc()</code> function from the modelr package.</p>
<pre class="r"><code>df_split &lt;-
  df %&gt;%
  crossv_mc(n = 50) %&gt;%
  print()</code></pre>
<pre><code># A tibble: 50 x 3
            train           test   .id
           &lt;list&gt;         &lt;list&gt; &lt;chr&gt;
1  &lt;S3: resample&gt; &lt;S3: resample&gt;    01
2  &lt;S3: resample&gt; &lt;S3: resample&gt;    02
3  &lt;S3: resample&gt; &lt;S3: resample&gt;    03
4  &lt;S3: resample&gt; &lt;S3: resample&gt;    04
5  &lt;S3: resample&gt; &lt;S3: resample&gt;    05
6  &lt;S3: resample&gt; &lt;S3: resample&gt;    06
7  &lt;S3: resample&gt; &lt;S3: resample&gt;    07
8  &lt;S3: resample&gt; &lt;S3: resample&gt;    08
9  &lt;S3: resample&gt; &lt;S3: resample&gt;    09
10 &lt;S3: resample&gt; &lt;S3: resample&gt;    10
# ... with 40 more rows</code></pre>
<p>We have 50 sets of splits; let’s have a closer look at one of the splits.</p>
<pre class="r"><code>str(df_split[1,])</code></pre>
<pre><code>Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:   1 obs. of  3 variables:
 $ train:List of 1
  ..$ :List of 2
  .. ..$ data:Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 100 obs. of  2 variables:
  .. .. ..$ x: num  0.521 0.463 0.524 0.989 0.542 ...
  .. .. ..$ y: num  1.86 1.57 1.68 1.95 1.84 ...
  .. ..$ idx : int  1 2 3 5 6 8 9 10 11 12 ...
  .. ..- attr(*, &quot;class&quot;)= chr &quot;resample&quot;
 $ test :List of 1
  ..$ :List of 2
  .. ..$ data:Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 100 obs. of  2 variables:
  .. .. ..$ x: num  0.521 0.463 0.524 0.989 0.542 ...
  .. .. ..$ y: num  1.86 1.57 1.68 1.95 1.84 ...
  .. ..$ idx : int  4 7 13 30 31 33 34 39 52 55 ...
  .. ..- attr(*, &quot;class&quot;)= chr &quot;resample&quot;
 $ .id  : chr &quot;01&quot;</code></pre>
<p>One thing you may notice is that both the train and test elements both <em>appear</em> to contain the entire original dataset. In fact, R keeps a single copy of the original dataframe, then uses pointers wherever it is used elsewhere in the session. For more, you can read <a href="http://adv-r.had.co.nz/memory.html#object-size">this section</a> and how I got some help in <a href="https://github.com/hadley/modelr/issues/14">this thread</a>.</p>
<p>We can demonstrate this using the <code>object_size()</code> function in the pryr package. Keep in mind that <code>df_split</code> contains 100 references to <code>df</code>.</p>
<pre class="r"><code>object_size(df)</code></pre>
<pre><code>2.5 kB</code></pre>
<pre class="r"><code>object_size(df_split)</code></pre>
<pre><code>55.8 kB</code></pre>
<p>The next step is to make a function that takes a dataframe and returns a model. A lot of things will be made easier if you make functions in advance - or make them later so that it appears that you made them in advance (only git will know). In our case, we are going to look at how a linear model, <em>linear</em> in <span class="math inline">\(x\)</span>. Keep in mind that the “truth” is quadratic in <span class="math inline">\(x\)</span>.</p>
<pre class="r"><code># given a dataframe, return a model
fn_model &lt;- function(df){
  lm(y ~ x, data = df)
}</code></pre>
<p>We use the <code>map()</code> function from the purrr package to make models based on our training sets and our model function.</p>
<pre class="r"><code>df_split_model &lt;-
  df_split %&gt;%
  mutate(model = map(train, fn_model)) %&gt;%
  print()</code></pre>
<pre><code># A tibble: 50 x 4
            train           test   .id    model
           &lt;list&gt;         &lt;list&gt; &lt;chr&gt;   &lt;list&gt;
1  &lt;S3: resample&gt; &lt;S3: resample&gt;    01 &lt;S3: lm&gt;
2  &lt;S3: resample&gt; &lt;S3: resample&gt;    02 &lt;S3: lm&gt;
3  &lt;S3: resample&gt; &lt;S3: resample&gt;    03 &lt;S3: lm&gt;
4  &lt;S3: resample&gt; &lt;S3: resample&gt;    04 &lt;S3: lm&gt;
5  &lt;S3: resample&gt; &lt;S3: resample&gt;    05 &lt;S3: lm&gt;
6  &lt;S3: resample&gt; &lt;S3: resample&gt;    06 &lt;S3: lm&gt;
7  &lt;S3: resample&gt; &lt;S3: resample&gt;    07 &lt;S3: lm&gt;
8  &lt;S3: resample&gt; &lt;S3: resample&gt;    08 &lt;S3: lm&gt;
9  &lt;S3: resample&gt; &lt;S3: resample&gt;    09 &lt;S3: lm&gt;
10 &lt;S3: resample&gt; &lt;S3: resample&gt;    10 &lt;S3: lm&gt;
# ... with 40 more rows</code></pre>
<p>For each train/test split we now have a model associated. Before moving on, there are two issues I have with using the <code>lm()</code> function in this context. One is that <code>lm()</code> is not, as they say, “data first” - the first argument is a formula, not a dataframe. This makes it less easy to use in this context (requiring a function to wrap it). This problem is being addressed by the <a href="https://github.com/rbertolusso/intubate">“intubate” package</a>. My second issue is that <code>lm()</code> saves the entire training dataset for each model, obviating the savings achieved in the previous step.</p>
<pre class="r"><code>object_size(df_split_model)</code></pre>
<pre><code>618 kB</code></pre>
<p>Mine is not the best judgement in this area, but it seems there are a couple of things that could be done - follow the approach used by the <a href="https://cran.r-project.org/web/packages/biglm/index.html">“biglm” package</a>, which uses a lightweight structure to store the model, or perhaps to adapt <code>lm()</code> to use a S3 resample object. OK, that second idea is a little unhinged.</p>
<p>This is all to say that the purpose here is different than the one for which <code>lm()</code> was originally written.</p>
<p>Onwards. For the next step, we need functions to determine, for a given sample and model, the response, the predicted value, and the residual.</p>
<pre class="r"><code># given a dataframe and a model, return a vector of residuals
sample_prediction &lt;- function(sample, model){
  
  df &lt;- as.data.frame(sample)
  
  pred &lt;- stats::predict(model, df)

  pred
}

# this will work for lm, biglm - will have to check for others
sample_response &lt;- function(sample, model){
  
  df &lt;- as.data.frame(sample)
  
  var_response &lt;- all.vars(formula(model))[[1]]
  
  df[[var_response]]
}</code></pre>
<p>This next set of steps is a bit ambitious. We want to make a tall dataframe, where the <code>split</code> becomes a variable that can take the values <code>train</code> or <code>test</code>. For each model-split combination, we make a set of predictions and responses. We unnest the dataframe, then for each observation, we calculate the residual.</p>
<pre class="r"><code>df_split_resid &lt;- 
  df_split_model %&gt;%
  gather(key = split, value = data, train, test) %&gt;%
  mutate(
    pred = map2(data, model, sample_prediction),
    resp = map2(data, model, sample_response)
  ) %&gt;%
  select(-model, -data) %&gt;%
  unnest() %&gt;%
  mutate(resid = pred - resp) %&gt;%
  print()</code></pre>
<pre><code># A tibble: 5,000 x 5
     .id split     pred     resp        resid
   &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;
1     01 train 1.706350 1.855529 -0.149178776
2     01 train 1.652638 1.573038  0.079599650
3     01 train 1.709516 1.677966  0.031550473
4     01 train 1.725347 1.835031 -0.109683252
5     01 train 1.848846 1.889039 -0.040193328
6     01 train 1.751219 1.759407 -0.008187925
7     01 train 1.938876 1.918960  0.019916145
8     01 train 1.428275 1.339989  0.088286744
9     01 train 1.785805 1.822474 -0.036668960
10    01 train 1.686278 1.733915 -0.047637221
# ... with 4,990 more rows</code></pre>
<p>The thing we are interested to do is to compare the performance of models on their trianing sets with their proformance on their test sets. One way we can do this is to make qualitative assesments of the kernel density of the residuals.</p>
<pre class="r"><code>ggplot(df_split_resid, aes(x = resid, color = split, group = paste(split, .id))) +
  stat_density(
    aes(y = ..density..), 
    geom = &quot;line&quot;, 
    position = &quot;identity&quot;,
    alpha = 0.3
  ) +
  guides(color = guide_legend(override.aes = list(alpha = 1)))</code></pre>
<p><img src="model_cv_selection_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>We can see that fitting models that are linear in <span class="math inline">\(x\)</span> to data that are quadratic in <span class="math inline">\(x\)</span> may be problematic. Although the kernel-density estimates for all the training residuals seem consistent one to another, the kernel-density estimates of the test sets seem inconsistent with those for the training sets, and with each other.</p>
<p>We can reduce the data for each training or test set to a single statistic. For regression problems, Hadley likes to use the RMSE, or root-mean-squared error because it gives us a useful first idea of how a given model performs, and it is expressed in the same units as the response.</p>
<pre class="r"><code>df_split_rmse &lt;- 
  df_split_resid %&gt;%
  group_by(.id, split) %&gt;%
  summarize(
    rmse = sqrt(sum(resid*resid)/length(resid))
  ) %&gt;%
  ungroup() %&gt;%
  print()</code></pre>
<pre><code># A tibble: 100 x 3
     .id split      rmse
   &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;
1     01  test 0.1623577
2     01 train 0.1042993
3     02  test 0.1154751
4     02 train 0.1189499
5     03  test 0.0981743
6     03 train 0.1232048
7     04  test 0.1217852
8     04 train 0.1174028
9     05  test 0.1061283
10    05 train 0.1212039
# ... with 90 more rows</code></pre>
<p>We can visualize this summary.</p>
<pre class="r"><code>ggplot(df_split_rmse, aes(x = &quot;linear&quot;, y = rmse)) +
  geom_point(
    aes(color = split), 
    position = position_jitterdodge(jitter.width = 0.2, dodge.width = 0.5),
    alpha = 0.75
  ) +
  ylim(0, NA) +
  guides(color = guide_legend(override.aes = list(alpha = 1)))</code></pre>
<p><img src="model_cv_selection_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>We can see some differences in the behavior of the test data, compared with the training data. It would be much more useful to make these comparisons for different models.</p>
</div>
<div id="multiple-models" class="section level3">
<h3>Multiple models</h3>
<p>The first thing we will need to do is to make a list of functions, each one to return a different model.</p>
<p>We are going to do is create a function that, given the order of the polynomial, returns a function to make that model (given some data). Got it?</p>
<pre class="r"><code>make_model &lt;- function(order){
  
  # superstition (maybe) to evaluate the expression, preserving it
  # in this environment for the function
  order
  
  function(df){
    
    df &lt;- as.data.frame(df)
    
    lm(y ~ poly(x, order), data = df)
  }
}</code></pre>
<p>Now we can make a dataframe of models and give each an appropriate name. The <code>frame_data()</code> function in the “tibble” package can be useful for making row-wise (artisnal) dataframes.</p>
<pre class="r"><code>df_model &lt;-
  frame_data(
    ~model_name, ~model_creator,
#    &quot;zero&quot;, make_model(0),
    &quot;one&quot;, make_model(1),
    &quot;two&quot;, make_model(2),
    &quot;three&quot;, make_model(3),
    &quot;four&quot;, make_model(4),
    &quot;five&quot;,  make_model(5)
  ) %&gt;% 
  print()</code></pre>
<pre><code># A tibble: 5 x 2
  model_name model_creator
       &lt;chr&gt;        &lt;list&gt;
1        one         &lt;fun&gt;
2        two         &lt;fun&gt;
3      three         &lt;fun&gt;
4       four         &lt;fun&gt;
5       five         &lt;fun&gt;</code></pre>
<p>Note to self, it would be interesting to have a function that takes a model as an input and returns a metric of complexity. Could be useful later.</p>
<p>Let’s join this model dataframe to the split-sample dataframe.</p>
<pre class="r"><code>df_sample_model &lt;- 
  expand.grid(.id = df_split$.id, model_name = df_model$model_name, stringsAsFactors = FALSE) %&gt;%
  as_data_frame() %&gt;%
  left_join(df_split, by = &quot;.id&quot;) %&gt;%
  left_join(df_model, by = &quot;model_name&quot;) %&gt;%
  print()</code></pre>
<pre><code># A tibble: 250 x 5
     .id model_name          train           test model_creator
   &lt;chr&gt;      &lt;chr&gt;         &lt;list&gt;         &lt;list&gt;        &lt;list&gt;
1     01        one &lt;S3: resample&gt; &lt;S3: resample&gt;         &lt;fun&gt;
2     02        one &lt;S3: resample&gt; &lt;S3: resample&gt;         &lt;fun&gt;
3     03        one &lt;S3: resample&gt; &lt;S3: resample&gt;         &lt;fun&gt;
4     04        one &lt;S3: resample&gt; &lt;S3: resample&gt;         &lt;fun&gt;
5     05        one &lt;S3: resample&gt; &lt;S3: resample&gt;         &lt;fun&gt;
6     06        one &lt;S3: resample&gt; &lt;S3: resample&gt;         &lt;fun&gt;
7     07        one &lt;S3: resample&gt; &lt;S3: resample&gt;         &lt;fun&gt;
8     08        one &lt;S3: resample&gt; &lt;S3: resample&gt;         &lt;fun&gt;
9     09        one &lt;S3: resample&gt; &lt;S3: resample&gt;         &lt;fun&gt;
10    10        one &lt;S3: resample&gt; &lt;S3: resample&gt;         &lt;fun&gt;
# ... with 240 more rows</code></pre>
<p>We now have a dataframe that has, for each combination of split and model-specification, a training set and a function that can be used to build a model. We make a convenience function to put our model-creator functions to use, then create our models:</p>
<pre class="r"><code>make_model &lt;- function(data, model_creator){
  model_creator(data)
}

df_sample_model_created &lt;-
  df_sample_model %&gt;%
  mutate(
    model = map2(train, model_creator, make_model)
  ) %&gt;%
  print()</code></pre>
<pre><code># A tibble: 250 x 6
     .id model_name          train           test model_creator    model
   &lt;chr&gt;      &lt;chr&gt;         &lt;list&gt;         &lt;list&gt;        &lt;list&gt;   &lt;list&gt;
1     01        one &lt;S3: resample&gt; &lt;S3: resample&gt;         &lt;fun&gt; &lt;S3: lm&gt;
2     02        one &lt;S3: resample&gt; &lt;S3: resample&gt;         &lt;fun&gt; &lt;S3: lm&gt;
3     03        one &lt;S3: resample&gt; &lt;S3: resample&gt;         &lt;fun&gt; &lt;S3: lm&gt;
4     04        one &lt;S3: resample&gt; &lt;S3: resample&gt;         &lt;fun&gt; &lt;S3: lm&gt;
5     05        one &lt;S3: resample&gt; &lt;S3: resample&gt;         &lt;fun&gt; &lt;S3: lm&gt;
6     06        one &lt;S3: resample&gt; &lt;S3: resample&gt;         &lt;fun&gt; &lt;S3: lm&gt;
7     07        one &lt;S3: resample&gt; &lt;S3: resample&gt;         &lt;fun&gt; &lt;S3: lm&gt;
8     08        one &lt;S3: resample&gt; &lt;S3: resample&gt;         &lt;fun&gt; &lt;S3: lm&gt;
9     09        one &lt;S3: resample&gt; &lt;S3: resample&gt;         &lt;fun&gt; &lt;S3: lm&gt;
10    10        one &lt;S3: resample&gt; &lt;S3: resample&gt;         &lt;fun&gt; &lt;S3: lm&gt;
# ... with 240 more rows</code></pre>
<p>Now, lets generate some residuals:</p>
<pre class="r"><code>df_sample_resid &lt;-
  df_sample_model_created %&gt;%
  gather(key = split, value = data, train, test) %&gt;%
  mutate(
    pred = map2(data, model, sample_prediction),
    resp = map2(data, model, sample_response)
  ) %&gt;%
  select(.id, model_name, split, pred, resp) %&gt;%
  unnest() %&gt;%
  mutate(resid = pred - resp) %&gt;%
  print()</code></pre>
<pre><code># A tibble: 25,000 x 6
     .id model_name split     pred     resp        resid
   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;
1     01        one train 1.706350 1.855529 -0.149178776
2     01        one train 1.652638 1.573038  0.079599650
3     01        one train 1.709516 1.677966  0.031550473
4     01        one train 1.725347 1.835031 -0.109683252
5     01        one train 1.848846 1.889039 -0.040193328
6     01        one train 1.751219 1.759407 -0.008187925
7     01        one train 1.938876 1.918960  0.019916145
8     01        one train 1.428275 1.339989  0.088286744
9     01        one train 1.785805 1.822474 -0.036668960
10    01        one train 1.686278 1.733915 -0.047637221
# ... with 24,990 more rows</code></pre>
<p>And look at some kernel-density distributions:</p>
<pre class="r"><code>df_sample_resid %&gt;%
  mutate(model_name = factor(model_name, levels = c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;, &quot;four&quot;, &quot;five&quot;))) %&gt;%
  ggplot(aes(x = resid, color = split, group = paste(split, .id))) +
  stat_density(
    aes(y = ..density..), 
    geom = &quot;line&quot;, 
    position = &quot;identity&quot;,
    alpha = 0.3
  ) +
  facet_wrap(~ model_name) +
  guides(color = guide_legend(override.aes = list(alpha = 1)))</code></pre>
<p><img src="model_cv_selection_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<pre class="r"><code>df_sample_rmse &lt;- 
  df_sample_resid %&gt;%
  group_by(.id, model_name, split) %&gt;%
  summarize(
    rmse = sqrt(sum(resid*resid)/length(resid))
  ) %&gt;%
  ungroup() %&gt;%
  print()</code></pre>
<pre><code># A tibble: 500 x 4
     .id model_name split       rmse
   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;
1     01       five  test 0.14853889
2     01       five train 0.09358149
3     01       four  test 0.15137501
4     01       four train 0.09367255
5     01        one  test 0.16235774
6     01        one train 0.10429925
7     01      three  test 0.14097677
8     01      three train 0.09409568
9     01        two  test 0.12855223
10    01        two train 0.09539883
# ... with 490 more rows</code></pre>
<pre class="r"><code>df_sample_rmse %&gt;%
  mutate(model_name = factor(model_name, levels = c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;, &quot;four&quot;, &quot;five&quot;))) %&gt;%
  ggplot(aes(x = model_name, y = rmse, color = split, group = split)) +
  stat_summary(geom = &quot;line&quot;, fun.y = &quot;median&quot;) +
  geom_point(
    position = position_jitterdodge(jitter.width = 0.2, dodge.width = 0.5),
    alpha = 0.5
  ) +
  ylim(0, NA) +
  guides(color = guide_legend(override.aes = list(alpha = 1)))</code></pre>
<p><img src="model_cv_selection_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>The “two” model is the winner.</p>
<p>The lines connect the median rmse for each model-type. I need to find a way to characterize the complexity of a given model so that the x-axis can be automatically sorted (maybe rmse on training error can be used to sort within groups of equally-complex models).</p>
<p>Some work to do, but I think this is headed somehwere.</p>
</div>
</div>
<div id="logistic-regression" class="section level2">
<h2>Logistic regression</h2>
</div>
<div id="session" class="section level2">
<h2>Session</h2>
<pre class="r"><code>session_info()</code></pre>
<pre><code>Session info --------------------------------------------------------------</code></pre>
<pre><code> setting  value                       
 version  R version 3.3.1 (2016-06-21)
 system   x86_64, darwin13.4.0        
 ui       X11                         
 language (EN)                        
 collate  en_US.UTF-8                 
 tz       America/Chicago             
 date     2016-08-03                  </code></pre>
<pre><code>Packages ------------------------------------------------------------------</code></pre>
<pre><code> package    * version    date       source                
 assertthat   0.1        2013-12-06 CRAN (R 3.3.0)        
 codetools    0.2-14     2015-07-15 CRAN (R 3.3.1)        
 colorspace   1.2-6      2015-03-11 CRAN (R 3.3.0)        
 DBI          0.4-1      2016-05-08 CRAN (R 3.3.0)        
 devtools   * 1.12.0     2016-06-24 CRAN (R 3.3.0)        
 digest       0.6.9      2016-01-08 CRAN (R 3.3.0)        
 dplyr      * 0.5.0      2016-06-24 cran (@0.5.0)         
 evaluate     0.9        2016-04-29 CRAN (R 3.3.0)        
 formatR      1.4        2016-05-09 CRAN (R 3.3.0)        
 ggplot2    * 2.1.0      2016-03-01 CRAN (R 3.3.0)        
 gtable       0.2.0      2016-02-26 CRAN (R 3.3.0)        
 htmltools    0.3.5      2016-03-21 CRAN (R 3.3.0)        
 knitr        1.13       2016-05-09 CRAN (R 3.3.0)        
 labeling     0.3        2014-08-23 CRAN (R 3.3.0)        
 lazyeval     0.2.0      2016-06-12 CRAN (R 3.3.0)        
 magrittr     1.5        2014-11-22 CRAN (R 3.3.0)        
 memoise      1.0.0      2016-01-29 CRAN (R 3.3.0)        
 modelr     * 0.0.0.9000 2016-07-13 github (hadley/modelr)
 munsell      0.4.3      2016-02-13 CRAN (R 3.3.0)        
 plyr         1.8.4      2016-06-08 cran (@1.8.4)         
 pryr       * 0.1.2      2015-06-20 CRAN (R 3.3.0)        
 purrr      * 0.2.2.9000 2016-07-13 github (hadley/purrr) 
 R6           2.1.2      2016-01-26 CRAN (R 3.3.0)        
 Rcpp         0.12.5     2016-05-14 CRAN (R 3.3.0)        
 rmarkdown    1.0        2016-07-08 CRAN (R 3.3.0)        
 scales       0.4.0      2016-02-26 CRAN (R 3.3.0)        
 stringi      1.1.1      2016-05-27 CRAN (R 3.3.0)        
 stringr      1.0.0      2015-04-30 CRAN (R 3.3.0)        
 tibble     * 1.1        2016-07-04 cran (@1.1)           
 tidyr      * 0.5.1      2016-06-14 cran (@0.5.1)         
 withr        1.0.2      2016-06-20 CRAN (R 3.3.0)        
 yaml         2.1.13     2014-06-12 CRAN (R 3.3.0)        </code></pre>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
